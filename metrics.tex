\section{Heuristics}\label{sec:heuristics}

We present a parametric heuristic (\cref{subsec:heuristics}) tailored to address the computational complexities associated with enumerating all possible combinations within a given set.
The primary aim of the heuristic is to approximate the optimal path for service interactions and transformations, particularly within the landscape of more complex pipelines composed numerous nodes and candidate services.

Within this context, we highlight the crucial role of well-defined metrics (\cref{sec:metrics}) in assessing data quality across the pipeline.
Inspired by existing literature, these metrics, categorized as quantitative and statistical, play a pivotal role in quantifying the impact of policy-driven transformations on the original dataset.
Our focus extends beyond identifying optimal combinations, encompassing an understanding of the quality changes introduced during the transformation processes.


\subsection{Quality Metrics}\label{sec:metrics}


Ensuring data quality is mandatory to implement high-value data pipelines, which provide high-quality results and decision-making along the whole pipeline execution.
A set of metrics must be defined to evaluate data quality at each step of the big data pipeline.

The metrics in this paper can be classified in \emph{quantitative} and \emph{statistical}~\cite{ADD}.
They evaluate the quality loss introduced by our policy-driven transformation in Section~\cite{ADD} on the original dataset \origdataset.
In other words, they compare original dataset \origdataset\ and transformed dataset \transdataset\ generated by enforcing data protection requirements on \origdataset.
Quantitative metrics monitor the amount of data lost during data transformation as the difference in quality between datasets \origdataset\ and \transdataset.
On the other hand, the statistical approach takes into consideration the changes in the statistical properties of dataset \origdataset\ and \transdataset.
We note that the metrics can be applied either to the entire dataset or specific features only.
The features can be assigned with equal or varying importance, enabling the prioritization of important features that might be possibly lost during the policy-driven transformation in Section~\cite{ADD}.

\subsubsection{Jaccard coefficient}
The Jaccard coefficient can be used to measure the difference between the elements in two datasets.
It is defined as:\[J(X,Y) = \frac{|X \cap Y|}{|X \cup Y|}\]
where X and Y are two datasets of the same size.

The Jaccard coefficient is computed by dividing the cardinality of the intersection of two sets by the cardinality of their union. It ranges from 0 to 1, where 0 indicates no similarity and 1 indicates complete similarity between the datasets.

The Jaccard coefficient has several advantages. Unlike other similarity measures, such as Euclidean distance, it is not affected by the magnitude of the values in the dataset. It is suitable for datasets with categorical variables or nominal data, where the values do not have a meaningful numerical interpretation.

\subsubsection{Weighted Jaccard coefficient}
The Jaccard coefficient can be extended with weights modeling the importance of each element in the dataset.
The Weighted Jaccard coefficient is defined as:\[J(X,Y) = \frac{\sum_{i=1}^{n}w_i(x_i \cap y_i)}{\sum_{i=1}^{n}w_i(x_i \cup y_i)}\]
where X and Y are two datasets of the same size.

It is computed by dividing the cardinality of the intersection of two datasets by the cardinality of their union, weighted by the importance of each element in the datasets. Weights prioritize certain elements (e.g., a specific feature) in the datasets.
%This approach is particularly useful when some elements in the dataset have more importance or relevance than others.
The Weighted Jaccard coefficent can then account for element importance and provide a more accurate measure of similarity.

\subsubsection{Jensen-Shannon Divergence}
The Jensen-Shannon divergence (JSD) is a symmetrized version of the KL divergence and can be used to measure the dissimilarity between the probability distributions of two datasets.

The JSD between X and Y can be calculated as:

\[JSD(X, Y) = \frac{1}{2} \left( KL(X || M)
  + KL(Y || M) \right)\]

where X and Y are two datasets of the same size, and M$=$0.5*(X+Y) is the average distribution.

The JSD incorporates both the KL divergence from X to M and from Y to M. It provides a balanced measure of dissimilarity that is symmetric and accounts for the contribution from both datasets.

The JSD can compare the dissimilarity of the two datasets, providing a symmetric and normalized measure that considers the overall distribution of the data.

JSD provides a more comprehensive understanding of the dissimilarity between X and Y, taking into account the characteristics of both datasets.

\subsubsection{Weighted Jensen-Shannon Divergence}

The Jensen-Shannon divergence (JSD) can be extended to incorporate weights in the calculation, resulting in a weighted version of the dataset divergence. The Weighted Jensen-Shannon divergence accounts for the importance or relevance of specific elements in the datasets being compared.

The weighted Jensen-Shannon divergence between X and Y, denoted as JSDw(X, Y), is defined as:
\begin{align*}
  JSDw(X, Y) = \frac{1}{2} \left( \sum_{i=1}^{n} w_i \left( x_i \log \frac{x_i}{m_i} \right) \right. \\
  \left. + \sum_{i=1}^{n} w_i \left( y_i \log \frac{y_i}{m_i} \right) \right)
\end{align*}

where X and Y are two datasets of the same size, x\_i$\in$X and y\_i$\in$Y are the elements of X and Y, respectively, w\_i represents the weights assigned to each element, and \[m_i = \frac{{x_i + y_i}}{2}\] is the average of the corresponding elements.

By incorporating weights into the JSD calculation, the Weighted Jensen-Shannon divergence provides a more accurate measure of dissimilarity between X and Y, considering the importance of individual elements based on the assigned weights. This approach is particularly useful when elements in the datasets have varying levels of significance, enabling a more tailored analysis of dissimilarity.



\subsection{Heuristic}\label{subsec:heuristics}
\hl{The computational challenge posed by the enumeration of all possible combinations within a given set is a well-established NP-hard problem.}
The exhaustive exploration of such combinations swiftly becomes impractical in terms of computational time and resources,
particularly when dealing with the analysis of complex pipelines.
In response to this computational complexity,
the incorporation of heuristic emerges as a strategy to try to efficiently address the problem.

In this article, our focus centers on a specific heuristic known as "sliding windows" to tackle the combination problem.
In \cref{subsec:slidingwindow}, we present more details about this heuristic presenting the pseudocode and an example of its application.
It is noteworthy to highlight that, within the sliding windows paradigm, a window size of 1 corresponds to the "greedy" approach,
while a window size of N, where N represents the number of nodes, is equivalent to the exhaustive method.
\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{commentsColor}\textit,    % comment style
  deletekeywords={list},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=tb,	                   	   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{keywordsColor}\bfseries,       % keyword style
  language=Python,                 % the language of the code (can be overrided per snippet)
  otherkeywords={*,to,function, Seq, add,empty},           % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{commentsColor}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=1,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{stringColor}, % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname,                  % show the filename of files included with \lstinputlisting; also try caption instead of title
  columns=fixed                    % Using fixed column width (for e.g. nice alignment)
}
% \subsection*{Greedy}
% The greedy algorithm is a heuristic that can be used to minimize the quantity of information lost by making locally optimal choices at each step in the hope of achieving a globally optimal solution.
% For instance, in our service selection problem, the greedy algorithm can be used to select services in order of their information loss, starting with the service with the lowest information loss.
% This strategy ensures that services with lower information loss are selected first, minimizing the overall quantity of information lost.
% Pseudo-code for the greedy algorithm is presented in Algorithm 1.
% \begin{lstlisting}[frame=single,caption={Greedy Heuristic Pseudocode},label={lst:greedy}]
% function GreedyHeuristic(Set nodes):

%   selectedNodes = empty
%   while nodes is not empty:
%     minMetricNode = None
%     minMetricValue = infinity
%     for node in nodes:
%       currentMetric = calculateMetric(node)
%       if currentMetric < minMetricValue:
%         minMetricValue = currentMetric
%         minMetricNode = node
%         add minMetricNode to selectedNodes
%         remove minMetricNode from nodes
%   return selectedNodes
%         \end{lstlisting}

% The pseudocode and is made of one function, GreedyHeuristic, which takes a set of nodes as input and returns a set of selected nodes.
% The function starts by initializing an empty set of selected nodes.
% Then, while there are still nodes to be selected, the algorithm iterates over the nodes and selects the node with the lowest metric value.
% The selected node is then added to the set of selected nodes and removed from the set of nodes.
% Finally, the set of selected nodes is returned.
\subsection*{Sliding Window}\label{subsec:slidingwindow}

The sliding window algorithm is a heuristic that can be used to minimize the quantity of information lost by considering a subset of the available services defined by a fixed or moving window.
For example, in our service selection problem where the quantity of information lost needs to be minimized,
the sliding window algorithm can be used to select services composition that have the lowest information loss within a fixed-size window.
This strategy ensures that only services with low information loss are selected, minimizing the overall quantity of information lost.
Pseudo-code for the sliding window algorithm is presented in Algorithm 1.
\begin{lstlisting}[frame=single, caption={Sliding Window Heuristic} ,label={lst:slidingwindow}]
function SlidingWindowHeuristic(Seq sequence, int windowSize):

  selectedNodes = empty
  for i from 0 to length(sequence) - windowSize:
    minMetricNode = None
    minMetricValue = infinity
    for j from i to i + windowSize:
      currentMetric = calculateMetric(sequence[j])
      if currentMetric < minMetricValue:
        minMetricValue = currentMetric
        minMetricNode = sequence[j]
    add minMetricNode to selectedNodes
  return selectedNodes
\end{lstlisting}
The pseudocode is made of one function, SlidingWindowHeuristic, which takes a sequence of nodes and a window size as input and returns a set of selected nodes.
The function starts by initializing an empty set of selected nodes.
Then, for each node in the sequence, the algorithm iterates over the nodes in the window and selects the node with the lowest metric value.
The selected node is then added to the set of selected nodes.
Finally, the set of selected nodes is returned.


The utilization of heuristic in service selection can be enhanced through the incorporation of techniques derived from other algorithms, such as Ant Colony Optimization or Tabu Search.
By integrating these approaches, it becomes feasible to achieve a more effective and efficient selection of services, with a specific focus on eliminating paths that have previously been deemed unfavorable.

%\AG{It is imperative to bear in mind that the merging operations subsequent to the selection process and the joining operations subsequent to the branching process are executed with distinct objectives. In the former case, the primary aim is to optimize quality, whereas in the latter, the foremost objective is to minimize it.}
