\section{Related Work}\label{sec:related}

{\color{OurColor}
We present an overview of the related work and a comparison with existing relevant tools and solutions in literature.
% In this related work section, we address two fundamental challenges in the field of data protection for service-based pipelines.

% In Section  \ref{sec:dataquality}, we address the issue of the lack of consensus on the definition of data quality and, consequently, on data quality metrics when applying data protection transformations. In Section \ref{sec:datagov}, we examine existing data governance solutions that tackle the problem of data protection when sharing data among different services.
}
%%%%%%%%%%%%%%%%%%%%
\subsection{Data quality and data protection}\label{sec:dataquality}
%%%%%%%%%%%%%%%%%%%%

%Data quality is a widely studied research topic across various communities and perspectives, such as the database community or when evaluating privacy preserving data mining techniques. In the context of big data, data quality primarily refers to the extent to which big data meets the requirements and expectations of its intended use, encompassing various dimensions and characteristics to ensure the data is reliable, accurate, and valuable for analysis and decision-making. Specifically, accuracy denotes the correctness of the data, ensuring it accurately represents the real-world entities and events it models.
Data quality is a widely studied research topic studied across various communities and perspectives. In the context of (big) data pipelines, data quality primarily refers to the extent to which (big) data meets the requirements and expectations of its intended use, encompassing various dimensions and characteristics to ensure the data are reliable, accurate, and valuable for analysis and decision-making. Specifically, accuracy denotes the correctness of the data, ensuring it accurately represents the real-world entities and events it models.
    {\color{OurColor}
        With the increasing need to protect sensitive data, the notion of data quality has expanded to include a broader concept of accuracy, particularly in terms of the proximity of a sanitized value to the original value.
        This shift has emphasized the need of metrics to assess the quality of data resulting from anonymization processes.
        Differential privacy \cite{dwork2008differential}, k-anonymity \cite{k-anon}, and l-diversity \cite{l-diversity} are three distinct techniques used to provide data anonymization, with different protection levels and results on data quality. For example, differential privacy is highly effective in maintaining confidentiality, but the noise added can reduce data precision, impacting analytical accuracy, whereas k-anonymity and l-diversity generally maintain higher data quality than differential privacy, but they might still be unable to protect against sophisticated attacks.
    }
Various data quality metrics have been proposed in existing literature, including generalized information loss (\textit{GenILoss}), discernability metric, minimal distortions, and average equivalence class size ($C_{AVG}$), which may either have broad applicability or be tailored to specific data scenarios \cite{Majeed2021AnonymizationTF,bookMetrics,reviewMetrics}. However, there is currently no metric that is widely accepted by the research community. The main challenge with data quality is its relative nature: its evaluation typically depends on the context in which the data is used and often involves both objective and subjective parameters \cite{dataAccuracy,dataQuality}.
%
A common consideration across all contexts is that accuracy is closely related to the information loss resulting from the anonymization strategy: the lower the information loss, the higher the data quality. In our scenario, we have opted for two generic metrics rooted in data loss assessment (i.e., data completeness) - one quantitative and one qualitative. Nonetheless, our framework and heuristic are designed to be modular and flexible, accommodating the chosen metric.

    {\color{OurColor} While existing techniques have provided sound and effective solutions that guarantees data quality and data protection, they often unsuit to scenarios aiming to maximize data quality while ensuring data protection, have limited expressiveness (e.g., the definition of $k$ when $k$-anonymity is used to protect data), are not applicable to pipelines orchestrating services owned by different providers. Our solution fills in the above gaps, by providing a framework for service-based data pipelines that support the selection of data processing services that maximize data quality, while upholding privacy and security requirements. Service selection is driven by high-expressive policies, where data transformations built on data protection techniques (e.g., $k$-anonymity) are applied to data before they are used in the pipeline.}

%%%%%%%%%%%%%%%%%%
\subsection{Data quality and data protection in service-based pipelines}\label{sec:datagov}
%%%%%%%%%%%%%%%%%%

As organizations increasingly realize the practical benefits and significant value of big data, they also acknowledge the limitations of current big data ecosystems, especially in terms of data governance. In this context, the need for privacy-aware systems enforcing sensitive data protection without compromising data quality throughout the entire data lifecycle arises. Recently, both industry and academia have begun to investigate the issue, recognizing the need of new security requirements \cite{Colombo:JournCybersec:2019} and the importance of addressing the conflict between the need to share and the need to protect information \cite{balancingact,VANDENBROEK2018330,balancingInMedicine,needtobalance,dataProtection}, from a data governance perspective \cite{al2018exploring,aissa2020decide}, and, more in general, to ensure compliance of (big) data pipelines with generic non-functional requirements \cite{ABBJ.ICWS2022,ABHKKS.BD2023}.

{\color{OurColor}The pipeline template proposed in this work addresses these challenges by enabling to express the security policies at the right level of granularity, considering individual services in the pipeline. It can also be easily mapped onto specific platforms, such as Apache-based systems, as we have demonstrated in \cite{medes2021}.
Table \ref{tab:comparative} provides a comparative analysis with relevant existing approaches, highlighting how few industrial solutions compare to our framework according to the following critical features:
\begin{itemize}
    \item \textbf{F1 -- Service-Based Pipeline Support in the Cloud-Edge Continuum:} The ability to effectively operate within distributed environments spanning cloud and edge infrastructure.
    \item \textbf{F2 -- Quality-Aware Service Selection Ensuring Data Protection:} The capacity to optimize service selection processes, maintaining data quality across the pipeline and ensuring robust data protection measures.
    \item \textbf{F3 -- Framework-Agnostic Data Protection:} The degree to which each solution is bound to specific data protection techniques.
    \item \textbf{F4 -- Policy Expressiveness:} The degree to which each solution supports fine-grained specification of policies or privacy measures.
\end{itemize}

\begin{table}[t!]
    \centering
    \caption{Comparative analysis with relevant existing approaches   \label{tab:comparative}}
    \renewcommand{\arraystretch}{1.5}
    \footnotesize{
        \begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}X>{\raggedright\arraybackslash}X>{\raggedright\arraybackslash}X>{\raggedright\arraybackslash}X>{\raggedright\arraybackslash}X}
            \toprule
            \textbf{Solution}                                     & \textbf{F1}                                       & \textbf{F2}                                                                   & \textbf{F3}                                 & \textbf{F4}                                                        \\
            \midrule
            \textbf{Microsoft Presidio \cite{microsoft_presidio}} & \cmark, can integrate within cloud-edge pipelines & \tmark, focuses on data redaction                                             & \cmark, compatible with diverse techniques  & \tmark, pre-built PII detectors with configurable policies         \\

            \textbf{Apache Ranger \cite{apache_ranger}} & \tmark, mostly limited to cloud settings & \xmark, provides access control rather than service optimization & \cmark, integrates with various techniques & \cmark; high expressiveness with fine-grained policy control \\

            \textbf{Google Cloud DLP \cite{google_cloud_dlp}}     & \cmark, primarily within Google Cloud             & \tmark, focuses on redaction and anonymization                                & \cmark, works across data types             & \tmark, flexible templates for data masking and redaction policies \\

            \textbf{AWS Macie \cite{aws_macie}}                   & \tmark, suited for AWS cloud infrastructure       & \tmark, prioritizes data protection                                           & \cmark, AWS-centric                         & \tmark, supports predefined PII types but less customizable        \\

            \textbf{IBM Guardium \cite{ibm_guardium}}             & \cmark, supports hybrid cloud and on-prem setups  & \xmark, focuses on monitoring and access control                              & \cmark, adaptable to multiple frameworks    & \cmark, extensive policy-based access control and monitoring       \\

            \textbf{Apache Sentry \cite{apache_sentry}}           & \tmark, Hadoop ecosystems                         & \xmark, static access control                                                 & \xmark, closely tied to Hadoop              & \tmark, supports column and row-level access control               \\

            \textbf{Our paper}                                    & \cmark, suitable for cloud-edge environments      & \cmark, selection of services that optimize quality while ensuring protection & \cmark, data-protection techniques agnostic & \cmark, high expressiveness with fine-grained policy control       \\

            \bottomrule
        \end{tabularx}
    }
    \caption{Comparative analysis with relevant existing approaches. Feature support is classified according to \cmark (fully supported), \tmark (partially supported or limited in scope), \xmark (not supported)}
    \label{tab:comparative}
\end{table}

According to Table \ref{tab:comparative}, most of competitor solutions have full support for F3, while no solution has full support for F2. All solutions provide partial or full support for F1 and F4, with F4 fully supported by just two of the competitors.  Microsoft Presidio aligns most closely with our approach, as it supports cloud-edge integration, offers compatibility with diverse techniques, and includes configurable policies for PII detection. However, our tool uniquely supports the optimization of data quality alongside privacy through a service selection feature and across the entire pipeline lifecycle. Additionally, unlike other solutions that are cloud-specific, our tool maintains compatibility across hybrid environments, addressing both cloud-edge and on-premise scenarios.}

Additional solutions address individual aspects of these requirements. For example, several proposals address data protection by implementing robust access control on big data platforms. Some approaches are platform-specific, tailored to single systems like MongoDB or Hadoop, and leverage the native access control features of these platforms \cite{rathore2017hadoop,anisetti2018privacy,FederationAC:Journ:2020,Sandhu:ABAC:2018,GuptaSandu:2017}. Other approaches focus on specific databases, such as NoSQL or graph databases, or specific types of analytical pipelines \cite{AConGraphDB:2021, AConMongoDB:2022, ABACforHBase:2019}. However, these solutions often rely on query rewriting mechanisms, resulting in high complexity and low efficiency. Some solutions are designed for specific scenarios, such as federated cloud environments, edge microservices, or IoT, and lack the flexibility to adapt to multiple contexts \cite{MultipartyAC:2019, IoTSecurity}.
%
The most similar to our approach are platform-independent solutions that adopt Attribute-Based Access Control (ABAC) \cite{XACML3.0} as a common underlying model, given its ability to support highly flexible and dynamic forms of data protection for business-critical data. {\color{OurColor}For instance, Hu et al. \cite{ HUFerraiolo:2014} introduced a generalized access control model for big data processing frameworks that can be extended to the Hadoop environment. However, their work discusses the issues only from a high-level architectural perspective and does not offer a tangible solution or address data quality issues.}

To conclude, the selection and composition of services, originally discussed in the Web service scenario, face additional challenges in the era of big data due to the volume and velocity of data, as well as to the heterogeneity of services, domains, and hosting infrastructures. Despite its critical nature, security is often one of the least considered metrics in service selection \cite{SELLAMI2020102732}. Even when security is taken into account, it is not always coupled with data quality.
Related work in this area includes approaches (e.g., \cite{secureWScomposition}) where Web services are composed according to the security requirements of both service requestors and providers. However, the range of expressible requirements is limited, such as the type of encryption algorithm or authentication method (e.g., SSO), and data sanitization is not considered. Thus, the selection algorithm is just a matching rather than a ranking with respect to a security metrics.
Another relevant study \cite{9844845} implements a certification-based service selection process, ranking services according to their certified non-functional properties and corresponding user requirements. In this approach, certified services are assumed to be functionally equivalent, offering the same functionality while meeting users' functional requirements.
The most relevant solution is \cite{SELLAMI2020102732}, where the authors address the challenges of big service composition, with reference to QoS and security issues. Similarly to what we do with our pipeline template, they define a quality model for big services by extending the traditional QoS model of Web services to include ``big data"-related characteristics, and Quality of Data (QoD) attributes, such as completeness, accuracy, and timeliness. To address security issues, in their model, each service is assigned a L-Severity level \cite{Lseverity} that represents the potential severity of data leakages when consuming its data chunks.
Their approach aims to select the optimal composition plan that not only maximizes QoS and QoD attributes such as timeliness (TL), completeness (CP), and consistency (CS), but it also minimizes L-Severity (LS), data sources and communication costs.
