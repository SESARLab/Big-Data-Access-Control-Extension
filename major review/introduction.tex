\section{Introduction}
{\color{OurColor}The wide success and adoption of cloud-edge infrastructures and their intrinsic multitenancy radically change the way in which distributed systems are developed, deployed, and executed, redefining IT scalability, flexibility, and efficiency.} Multitenancy in fact enables multiple users to share resources, such as computing power, storage, and services, optimizing their utilization and reducing operational costs.

{\color{OurColor}In this scenario, the increasing ability of collecting and managing huge volume of data, coupled with a paradigm shift in service delivery models, has also significantly enhanced scalability and efficiency in data analytics. Data are treated as digital products, which are managed and analyzed by multiple services orchestrated in pipelines. This shift is also fostering the emergence of new platforms and environments, such as data marketplaces and data spaces, where data in critical domains (e.g., justice, healthcare, transportation) can be pooled and shared to maximize data quality and trustworthiness,
and distributed data management systems support data storing, versioning, and sharing for complex analytics processes.}\footnote{\url{https://joinup.ec.europa.eu/collection/elise-europeanlocation-
interoperability-solutions-e-government/glossary/
term/data-marketplace}, \url{https://internationaldataspaces.org/}, \url{https://digitalstrategy.
ec.europa.eu/en/library/staff-working-documentdata-spaces}}

%Data spaces expand upon this concept to define a complete environment where data infrastructures and governance frameworks are integrated to streamline data management. Within these environments, data consumers and providers collaborate to surmount existing legal and technical barriers to data sharing, thereby unlocking the full potential of their data.3


The flip side of a scenario {\color{OurColor}where service-based data pipelines orchestrate services selected at run time and are delivered in the cloud-edge continuum} is the increased complexity in data governance. {\color{OurColor}Data are shared and analyzed by multiple services owned by different providers introducing unique security challenges. On one side, the pipeline owner and data providers have different security requirements, access policies, and data sensitivity that vary according to the specific orchestrated services; on the other side, orchestrated services (data consumers) have different profiles that impact on the amount of data they can access and analyze.}

Adequate measures such as encryption, access control mechanisms, and data anonymization techniques  have been implemented to protect data against unauthorized access and ensure compliance with regulatory requirements such as GDPR~\cite{EuropeanParliament2016a} or HIPAA~\cite{hipaa1996}. {\color{OurColor}However, data quality is also crucial and must be guaranteed, as the removal or alteration of personally identifiable information from datasets to safeguard individuals' privacy can compromise the accuracy of analytics results.}

So far, all research endeavors have been mainly concentrated on exploring these two issues separately: on one hand, \emph{data quality}, encompassing accuracy, reliability, and suitability, has been investigated to understand the implications in analytical contexts~\cite{ANTONGIACOMO}.  On the other hand, \emph{data security and privacy} focused on the protection of confidential information and adherence to rigorous privacy regulations~\cite{ANTONGIACOMO}. {\color{OurColor}Although extensively studied, these investigations often prioritize enhancing the quality, security, and privacy of source data rather than ensuring data quality, security, and privacy throughout the entire processing pipeline, or the integrity of outcomes derived from data.}

A valid solution requires a holistic approach that integrates technological solutions, organizational policies, and ongoing monitoring and adaptation to emerging threats and regulatory changes {\color{OurColor}across the entire pipeline lifecycle.} The implementation of robust access control mechanisms or privacy techniques, ensuring that only authorized users can access specific datasets (or a portion thereof) is just a mandatory but initial step.
%
Additional requirements are emerging. First, data protection requirements should be identified at each stage of the data {\color{OurColor}pipeline}, potentially integrating techniques like data masking and anonymization (e.g., $k$-anonymity, $l$-diversity, differential privacy) to safeguard sensitive information, thereby preserving data privacy while enabling high quality data sharing and analysis. Second, data lineage should be prioritized, fostering a comprehensive understanding and optimization of data flows and transformations within complex analytical ecosystems. {\color{OurColor}Third, data protection and data quality requirements should drive the process that builds a pipeline with maximum data quality, while addressing data protection requirements.}

When evaluating a solution meeting the above criteria, the following questions naturally arise:
\begin{enumerate}
\item {\color{OurColor}How does a data protection solution affect data quality in the pipeline? How can we minimize this impact thus maximizing the overall data quality?}
\item Should data protection be implemented at each pipeline step rather than filtering all data at the outset?
\item {\color{OurColor}In a scenario where service-based data pipelines are built by selecting the best services among various candidate services, how might these choices be driven by quality requirements?}
\end{enumerate}

Based on the aforementioned considerations, we propose a data governance framework for {\color{OurColor}service-based data pipelines}. %, designed to mitigate privacy and security risks.
The primary objective of this framework is to support the selection of data processing services within the pipeline, with a central focus on the selection of those services that {\color{OurColor}maximize} data quality, while upholding privacy and security requirements.\footnote{{\color{OurColor}We note that the assembly of the selected services in an executable pipeline is out of the scope of this paper. Our approach however is agnostic to the specific executable environment.}}
To this aim, each element of the pipeline is \textit{annotated} with \emph{i)} data protection requirements expressing transformation on data and \emph{ii)} functional specifications on services expressing data manipulations carried out during each service execution.
Though applicable to a generic scenario, our data governance approach starts from the assumption that maintaining a larger volume of data leads to higher data quality; as a consequence, its service selection algorithm focuses on maximizing data quality {\color{OurColor}in terms of its completeness} by retaining the maximum amount of information when applying data protection transformations.

The primary contributions of the paper can be summarized as follows:
\begin{enumerate*}
  \item we define a data governance framework {\color{OurColor}that implements an algorithm for the selection of data processing services} enriched with metadata that describe both data protection and functional requirements;
  \item we propose a parametric heuristic tailored to address the computational complexity of the NP-hard service selection problem {\color{OurColor}that maximizes the quality of data, while addressing data protection and functional requirements};
  \item we evaluate the performance and quality of the algorithm through experiments conducted using {\color{OurColor}an open dataset from the domain of justice. Performance and quality are compared against two baselines modeling current approaches in literature.}
\end{enumerate*}

The remainder of the paper is structured as follows: Section 2, presents our system model, illustrating a reference scenario where data are owned by multiple organizations. Section \ref{sec:template} introduces the pipeline template and describe data protection and functional annotations. Section \ref{sec:instance} describes the process of building a pipeline instance from a pipeline template according to service selection. Section \ref{sec:heuristics} introduces the quality metrics used in service selection and the heuristic solving the service selection problem. Section \ref{sec:experiment} presents our experimental results. Section \ref{sec:related} discusses the state of the art and Section \ref{sec:conclusions} draws our concluding remarks.