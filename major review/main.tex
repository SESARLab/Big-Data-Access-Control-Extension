\documentclass[sn-mathphys-num,referee]{sn-jnl}
\usepackage{natbib} % For citations

%\usepackage{cite}
\usepackage{graphicx}
\usepackage{psfrag}
\usepackage{anyfontsize}
\usepackage{url}
\usepackage{color}
\usepackage{balance}
\usepackage[all]{xy}
\usepackage{xspace}
\usepackage{listings}
\usepackage{adjustbox}
\usepackage{array,booktabs,arydshln,xcolor}
\usepackage{tikz}
\usetikzlibrary{shapes,patterns,calc,shapes.geometric,arrows,positioning,backgrounds}
\usepackage{float}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{url}
\usepackage{amsthm}
\usepackage{float}
\usepackage{amsmath}
\usepackage[capitalise]{cleveref}
\usepackage{soul}
\usepackage[inline]{enumitem}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{epstopdf}
\usepackage{subcaption}
\colorlet{OurColor}{blue}
\graphicspath{{Images/}}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}[section]

\input{macro}

\begin{document}

\title[Maximizing Data Quality While Ensuring Data Protection in Service-Based Data Pipelines]{Maximizing Data Quality While Ensuring Data Protection in Service-Based Data Pipelines}
% \title[Maximizing Data Quality While Ensuring Data Protection in Service-Based Data Pipelines]{Maximizing Data Quality While Ensuring Data Protection in Service-Based Data Pipelines}
% \title[Service-Based Data Pipelines: Maximizing Data Quality While Ensuring Data Protection Requirements]{Service-Based Data Pipelines: Maximizing Data Quality While Ensuring Data Protection Requirements}
% \title[Service-Based Data Pipelines: Maximizing Data Quality While Ensuring Data Protection]{Service-Based Data Pipelines: Maximizing Data Quality While Ensuring Data Protection}
\keywords{Access Control, Big Data, Data Pipelines, Data Protection, Data Quality, Privacy}
%Data Transformation, Data Ingestion}

\author[1]{\fnm{Antongiacomo} \sur{Polimeno}}\email{antongiacomo.polimeno@unimi.it}
\author[1]{\fnm{Chiara} \sur{Braghin}}\email{chiara.braghin@unimi.it}
\author[1]{\fnm{Marco} \sur{Anisetti}}\email{marco.anisetti@unimi.it}
\author*[1]{\fnm{Claudio A.} \sur{Ardagna}}\email{claudio.ardagna@unimi.it}

\affil[1]{\orgdiv{Dipartimento di Informatica}, \orgname{Universit√† degli Studi di Milano}, \orgaddress{\city{Milano}, \country{Italy}}}

\maketitle

\begin{abstract}
~Today, the increasing ability of collecting and managing huge volume of data, coupled with a paradigm shift in service delivery models, has significantly enhanced scalability and efficiency in data analytics, particularly in multi-tenant environments. Data are today treated as digital products, which are managed and analyzed by multiple services orchestrated in data pipelines. {\color{OurColor}This paradigm shift towards distributed systems built as service-based data pipelines does not find any counterparts in the definition of new data governance techniques that properly manage data across the pipeline lifecycle, calling for innovative solutions to data pipeline management that primarily seek to balance data quality and data protection. Departing from the state of the art that traditionally targets single services and systems, and optimizes data protection and data quality as independent factors}, we propose a framework that enhances service selection and composition in service-based data pipelines to the aim of maximizing data quality, while providing a minimum level of data protection. Our approach first retrieves a set of candidate services compatible with data protection requirements in the form of access control policies; it then selects the subset of compatible services, to be integrated within the data pipeline, which maximizes the overall data quality. Being our approach NP-hard, a sliding-window heuristic is defined and experimentally evaluated in terms of performance and quality with respect to the exhaustive approach. Our results demonstrate a significant reduction in computational overhead, while maintaining high data quality.
\end{abstract}

\tikzset{
  do path picture/.style={%
      path picture={%
          \pgfpointdiff{\pgfpointanchor{path picture bounding box}{south west}}%
          {\pgfpointanchor{path picture bounding box}{north east}}%
          \pgfgetlastxy\x\y%
          \tikzset{x=\x/2,y=\y/2}%
          #1
        }
    },
  cross/.style={do path picture={
          \draw [line cap=round ] (-1,-1) -- (1,1) (-1,1) -- (1,-1);
        }},
  plus/.style={do path picture={
          \draw [line cap=round] (-3/4,0) -- (3/4,0) (0,-3/4) -- (0,3/4);
        }}
}

\input{introduction}
\input{system_model}
\input{pipeline_template.tex}
\input{pipeline_template_example.tex}
\input{pipeline_instance.tex}
\input{pipeline_instance_example.tex}
\input{metrics}
\input{experiment}
\input{related}


\section{Conclusions}\label{sec:conclusions}
In the realm of distributed data service pipelines, managing pipelines while ensuring both data quality and data protection presents numerous challenges. This paper proposed a framework specifically designed to address this dual concern. Our data governance model employs policies and continuous monitoring to address data security and privacy challenges, while preserving data quality, in service pipeline generation. The key point of the framework is in its ability to annotate each element of the pipeline with specific data protection requirements and functional specifications, then driving service pipeline construction. This method enhances compliance with regulatory standards and improves data quality by preserving maximum information across pipeline execution. Experimental results confirmed the effectiveness of our sliding window heuristic in addressing the computationally complex NP-hard service selection problem at the basis of service pipeline construction. Making use of a realistic dataset, our experiments evaluated the framework's ability to sustain high data quality while ensuring robust data protection, which is essential for pipelines where both data utility and privacy must coexist. To fully understand the impact of dataset selection on the retrieved quality and to ensure heuristic robustness across various scenarios, further investigation is planned for our future work. Future work will then %validate the findings of this paper and
explore deeper insights into the applicability of our heuristics across different scenarios.
{\color{OurColor}
  \subsection{Future Works}
Building upon the foundations established in this research, we have identifiedsome directions for future investigation. In the following we outline key areas for future research.
\begin{description}
  \item[Multidimensional Quality] Throughout the framework's development, we deliberately simplified our approach by representing quality through a single metric to maintain conceptual clarity. Future work will extend this foundation by addressing the inherent multidimensional nature of data quality. This expansion will enable more sophisticated monitoring and optimization mechanisms throughout the data lifecycle. Such enhancement will allow for the evaluation of various quality dimensions (e.g., completeness, timeliness, accuracy) weighted according to user priorities or task-specific requirements.
  \item[Additional Analysis] Future experimental investigations will include a deeper evaluation of how diverse datasets, an expanded range of services, and varying node configurations impact the established metrics. This extended analysis will include more realistic anonymization simulations and new metrics for assessing data completeness. The primary objective of these investigations is to identify generalizable patterns and recurring schemes that transcend specific experimental settings, thereby enhancing the broader applicability of our findings.
  \item[Real world scenario] Future research directions will focus on deploying and evaluating the proposed approach in real-world scenarios, enabling comprehensive assessment under real datasets and conditions. This practical deployment phase aims to validate theoretical foundations while revealing optimization opportunities . Additionally, empirical data from end-users will provide insights for refining the system's architecture, bridging the gap between theoretical efficacy and practical utility.
  \item[Data quality and quality of service] Beyond data quality monitoring, future research could extend the methodology to encompass service quality assessment. This integration would complement data quality evaluation with traditional service quality metrics, enabling the development of hybrid scenarios. Such scenarios would facilitate the selection of services that optimize quality while maintaining specific non-functional requirements (e.g., execution time, resource consumption).
\end{description}
}
\input{declarations}

\clearpage
%\bibliographystyle{spbasic}      % basic style, author-year citations
%%%%%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{bib_on_BigDataAccessControl}   % name your BibTeX data base

\end{document}

