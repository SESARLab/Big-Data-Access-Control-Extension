\subsection{Example}\label{sec:example}
\newcommand{\pone}{$\langle service\_owner=dataset\_owner\rangle$}
\newcommand{\ptwo}{$\langle service\_owner=partner(dataset\_owner) \rangle$}
\newcommand{\pthree}{$\langle service\_owner \neq dataset\_owner AND owner \neq partner(dataset\_owner)$}

\begin{table*}[ht!]
  \def\arraystretch{1.5}
  \centering
  \caption{Anonymization policies}\label{tab:anonymization}



  \begin{tabular}[t]{c|c|l}
    \textbf{Vertex}      & \textbf{Policy} & \policy{subject}{object}{action}{environment}{transformation}                          \\ \hline
    \vi{1},\vi{2},\vi{3} & $\p{0}$         & \policy{ANY}{dataset}{READ}{ANY}{\tp{0}}                                               \\
    \vi{4},\vi{6}        & $\p{1}$         & \policy{\pone}{dataset}{READ}{ANY}{\tp{0}}                                             \\
    \vi{4},\vi{6}        & $\p{2}$         & \policy{\ptwo}{dataset}{READ}{ANY}{\tp{1}}                                             \\
    %\vi{4},\vi{6}        & $\p{3}$         & \policy{\pthree}{dataset}{READ}{ANY}{\tp{2}}                                                    \\
    \vi{5}               & $\p{4}$         & \policy{ANY}{dataset}{READ}{ANY}{\tp{2}}                                               \\
    \vi{7}               & $\p{5}$         & \policy{$\langle service\_region= dataset.origin\rangle$}{dataset}{WRITE}{ANY}{\tp{0}} \\
    \vi{7}               & $\p{6}$         & \policy{$\langle service\_region=``\{CT,NY,NH\}"\rangle$}{dataset}{WRITE}{ANY}{\tp{1}} \\
    \vi{8}               & $\p{7}$         & \policy{ANY}{dataset} {READ}{$risky\_environment = risky$}{\tp{3}}                     \\
    \vi{8}               & $\p{8}$         & \policy{ANY}{dataset} {READ}{$risky\_environment = not\_risky$}{\tp{4}}                \\
  \end{tabular}

\end{table*}
\begin{table*}[ht!]
  \def\arraystretch{1.5}
  \centering
  \caption{Anonymization levels}\label{tab:levels}
  \begin{tabular}[t]{c|c|l}
    \textbf{\tf{i}} & \textbf{Level} & \textbf{Columns Anonymized}                       \\\hline
    \tp{0}          & Level0         & $anon(\varnothing)$                               \\
    \tp{1}          & level1         & $anon(FIRST\_NAME, LAST\_NAME)$                   \\
    \tp{2}          & level2         & $anon(FIRST\_NAME, LAST\_NAME, IDENTIFIER, AGE)$  \\
    \tp{3}          & risk0          & $aggregation(cluster=\infty)                    $ \\
    \tp{4}          & risk1          & $aggregation(cluster=10)                       $  \\
  \end{tabular}


\end{table*}

We present an example of pipeline template consisting of five stages, each stage is annotated with a policy as presented in \cref{tab:anonymization}.
We recall that \cref{tab:dataset} shows a sample of our reference dataset.

Let us consider the case study in \cref{sec:systemmodel}, a possibile pipeline template is shown in \cref{fig:service_composition_template}.
% 1° NODO %
The first stage consists of three parallel vertices (\vi{1}, \vi{2}, \vi{3}) and focuses on data collection.
The policy annotation \p{0} is linked with an empty transformation.
The functional requirement necessitates a URI as input, and the output is the downloaded dataset.

The second stage incorporates a sole vertex (\vi{4}),
which merges the three datasets obtained from the previous stages and is associated with three policies (\p{1},\p{2},\p{3}).
The policies are evaluated during the node execution:
% 2° NODO %
if the service owner is the same as the dataset owner  (\pone), the dataset is not anonymized (\tp{0}).
if the service owner is a partner of the dataset owner (\ptwo), the dataset is anonymized level1 (\tp{1}).
%if the service owner is neither the dataset owner nor a partner of the dataset owner  (\pthree), the dataset is anonymized level2 (\tp{2}).
The functional requirement specifies $n$ datasets as input, and the output is the merged dataset.
% 3° NODO %
The third stage, is responsible both for data analysis/statistics and machine learning tasks.
The stage is composed of two alternative vertices respectively \vi{4}, \vi{5}.

Data analytics vertex adopts policies analogous to the second stage. The logic remains consistent:
if the service profile matches with the data owner (\pone), \p{1} matches and level0 anonymization is applied (\tp{0});
if the service profile matches with a partner of the owner (\ptwo), \p{2} matches and level1 anonymization is applied  (\tp{1});
if the service profile doesn't match with a partner nor with the owner (\pthree), \p{3} matches and level2 anonymization is applied (\tp{2}).
The functional requirement specifies a dataset as input, and the output is the computed statistics.
% 4° NODO %
Machine Learning vertex adopts always a level2 anonymization (\p(4)) to prevent personal identifiers from entering into the machine learning algorithm/model (\tp{2}).
The functional requirement specifies a dataset as input, and the output is the trained model or an inference.
% 5° NODO %
The fifth stage manages data storage.
If the service is within the facility itself ($\langle service,region=FACILITY"\rangle$), \p{5} is satisfied, resulting in data anonymization level1 (\tp{1}).
Otherwise, if the service is in a partner region ($\langle service,region={CT,NY,NH}"\rangle$), the data undergo anonymization level2 (\tp{2}).
The functional requirement specifies the data to be provided as input, and the output is the URI of the stored data.
% 6° NODO %
The sixth stage is responsible for data visualization.
As stated in policy annotation \p{7} and \p{8}, the data is anonymized according to the environment in which the service is executed,
where risky environment is defined as a region outside the facility or a partner facility.
If the environment is risky (\p{7}), the data is anonymized risk0 (\tp{3}).
Otherwise, if the environment is not risky (\p{8}), the data is anonymized risk1 (\tp{4}).

The functional requirement specifies a dataset as input, and the output is the visualization of the data.

%In summary, this tion has delineated a comprehensive pipeline template. This illustrative pipeline serves as a blueprint, highlighting the role of policy implementation in safeguarding data protection across diverse operational stages.
