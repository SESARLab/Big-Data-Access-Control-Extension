\section{Coalition Building}\label{sec:coalition}
Coalition building is a crucial process having direct impact on the quality of the analytics results. Figure~\ref{fig:smet} shows how data lineage is impacted by the processing lineage and in particular by i) the \textit{coalition a\-greement} $\textit{CA}_C$ (i.e., the CA-driven transformations adopted for a give coalition) and by ii) the transformation produced by the different jobs (job-specific transformation) part of a given coalition \coalition{}.
Let us consider job $\job{1}^{\org{1}}$ of Figure~\ref{fig:smet} it receives as input the data \trans{1}(\dataset{1}) based on the dataset obtained by \dataset{1} after the transformation  \trans{1} which is associated to the data lineage by our AC model. It then produce a data that is the job-specific transformation on the input data (i.e., \trans{1}(\dataset{1})) generating \dataset{2}.
We note that our Big Data Analytics pipeline models includes alternatives allowing different processing lineage (linear independent path in the Big data graph G) doing the same analytics but using different jobs (e.g., a lineage including k-means or a lineage using c-means). This will lead to different job-specific transformation on the data for the same Big Data pipeline.
In this paper, for the sake of simplicity we i) consider different coalitions for each processing lineage, ii) coalitions made of trustworthy organizations \org{i} providing candidate services for each job and iii) job-specific transformation not influenced by the organizations' behavior.  
In this scenario, since any coalition of a given processing lineage will produce the same job-specific data transformation, the analytics pipeline quality is impacted only by the \textit{coalition agreement} $\textit{CA}_C$ or rather by the transformations \trans{i} imposed by the given coalition \coalition{} on the data lineage. 
In the following we first present metrics to evaluate data quality across the data lineage, and then a set of solutions to build coalitions for  given Big Data pipeline ensuring a given data quality.

%\begin{example}\label{ex:p1j}
%The choice of the specific deployment has an impact on the way in which the coalition \coalition{} of organizations \org{i} is formed as discussed in the following of this section.
%Let us consider the following example where we have a pipeline made of just one ingestion job that can be offered by service provider $s_1$ or by the service provider $s_2$. In case the $s_1$ is selected the transformation $T_1$ is triggered according to the authorization $s_1$ has on the data, in this example $s_1$ has full control meaning that transformation $t_1$ is empty. In case the $s_2$ is selected the transformation $T_2$ is triggered according to the authorization $s_2$ has on the data and in this example data labelled as PII are removed. 
%\end{example}
%Considering the two data lineage generated by the two different coalition in Example\ref{ex:p1j} the one involving $s_2$ produce a significant changes to data compared to the other one. This data changes can have direct impact on the quality of the analytics outcomes, therefore our goal is to build coalitions ensuring specific data quality. This coalition building problem can be assimilated to xxx showing an exponential complexity ...
%In the following we fist introduce our data quality metrics and then our euristics to solve the problem of coalition building

%\subsection{Data Quality metrics}
%\subsection{Coalition Heuristics}
\subsection{Metrics}

Data quality is a largely studied topic for the database management research communities, and is in general focused on the quality of the data source rather then on the quality of the data outcomes or of the data while used in the processing pipeline. In \cite{BigDataQaulitySurvey} a survey on big data quality is proposed mentioning the well known categories of big data quality grouped by intrinsic, contextual representational and accessibility categories. It also presents an holistic quality management model where the importance of data quality during processing is just mentioned in terms of requirements for the pre-processing job (e.g., data enhancement due to cleaning jobs). In this paper we depart from this idea on data quality at pre processing time only measuring it at each step of the big data pipeline.
%data quality are divided into four categories: intrinsic, contextual representational and accessibility that covers almost all the aspects of data at ingestion time



\begin{itemize}
    \item quanto l'applicazione delle tecniche di AC cambia il dataset o i dataset
    \item ad esempio parametri quantitativi sul dataset (cardinalitùà)
    \item parametri statistici del dataset
    \item pensieri di Ernesto
    \begin{itemize}
        \item In sede di ingestion, la valutazione sull’impatto dell’analitica la posso fare. Se privacy e valori numerici si può compiare epsilon e delta di differential privacy (noise addition o allargare la distribuzione di rumore sulla base del budget di privacy). 
        \item Metriche quantitative, come ricercar potrebbe essere interessante schema di budget di privacy per l’offuscamento di dati non numerici Quante volte il dato viene trasformato
        \item Analitiche di statistica descrittiva per il 3sigma -> piattaforma di puliafito (Catania)
        \item Anomaly detection con deviazione standard
        \item Data sanitizzato fa il merge di due barre dell’istogramma è un’autoencoding, come cambia la deviazione standard
    \end{itemize}
\end{itemize}