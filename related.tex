\section{Related Work}\label{sec:related}

%%%%%%%%%%%%%%%%%%%%
\subsection{Data quality}
%%%%%%%%%%%%%%%%%%%%

Data quality is a largely studied research topic, from different communities. 

to perform the data mining tasks in
a privacy-preserving way. These techniques for performing privacy-preserving
data mining are drawn from a wide array of related topics such as data mining,
cryptography and information hiding.

The main problem with data quality is that its evaluation is relative [18], in
that it usually depends on the context in which data are used.
non c'è un unica metrica, ci sono diverse metriche 

In our selection we tried to choose una che potesse andare bene. E soprattutto abbiamo cercato di fare delle considerazioni rispetto a tutto il ciclo di vita del dato

citare nostro journal


The main feature of the most PPDM algorithms is that they usually modify
the database through insertion of false information or through the blocking of
data values in order to hide sensitive information. Such perturbation techniques
cause the decrease of the data quality. It is obvious that the more the changes
are made to the database, the less the database reflects the domain of interest.
Therefore, data quality metrics are very important in the evaluation of PPDM
techniques.
In existing works, several data quality metrics have been proposed that are
either generic or data-use-specific. However, currently, there is no metric that
is widely accepted by the research community.
In evaluating the data quality after the privacy preserving process, it can be
useful to assess both the quality of the data resulting from the PPDM process
and the quality of the data mining results.
The quality of the data themselves
can be considered as a general measure evaluating the state of the individual
items contained in the database after the enforcement of a privacy preserving
technique. The quality of the data mining results evaluates the alteration in the
information that is extracted from the database after the privacy preservation
process, on the basis of the intended data use.
The main problem with data quality is that its evaluation is relative [18], in
that it usually depends on the context in which data are used.

In the scientific literature data quality is generally
considered a multi-dimensional concept that in certain contexts involves
both objective and subjective parameters [3, 34]. Among the various possible
parameters, the following ones are usually considered the most relevant:
- Accuracy: it measures the proximity of a sanitized value to the original
value.
- Completeness: it evaluates the degree of missed data in the sanitized
database.
- Consistency: it is related to the internal constraints, that is, the relationships
that must hold among different fields of a data item or among data
items in a database.

Accuracy. The accuracy is closely related to the information loss resulting
from the hiding strategy: the less is the information loss, the better is the
data quality. This measure largely depends on the specific class of PPDM algorithms.
In what follows, we discuss how different approaches measure the
accuracy.



The database management research community mainly focused on increasing the quality of the source data rather than guaranteeing data quality along the whole processing pipeline or the quality of outcomes built on data.
In \cite{BigDataQaulitySurvey}, a survey on big data quality is proposed mentioning the well known categories of big data quality grouped by intrinsic, contextual representational and accessibility categories.
It also presents an holistic quality management model where the importance of data quality during processing is just mentioned in terms of requirements for the pre-processing job (e.g., data enhancement due to cleaning jobs).
In this paper we depart from this idea on data quality at pre processing time only measuring it at each step of the big data pipeline.

tecniche di crittografia
\cite{8863330} \cite{Majeed2021AnonymizationTF}

cose simile a noi, nel senso che si rendono conto del problema e lo studiano
Privacy and analytics can work in tandem, but the mining outcome of a privacy aware design suffers from data quality
 in quello che segue ci sono anche in una tabella gli articoli citati
 NON SO SE METTERLI QUI O NELLA SEZIONE DOPO
 \cite{10.1007/978-981-15-0372-6_19}

%%%%%%%%%%%%%%%%%%
\subsection{Data protection}
%%%%%%%%%%%%%%%%%%

Research on data governance and protection focuses on the definition of new approaches and techniques aimed to protect the security and privacy of big data (e.g., CIA triad), as well as managing their life cycle with security and privacy in mind. Often, the research community is targeting specific security and privacy problems, resulting in a proliferation of solutions and tools, which are difficult to integrate in a coherent framework. Many solutions have been developed to protect the users' identity (e.g., anonymity \cite{wallace1999anonymity}, pseudonimity \cite{pfitzmann2001pseudonymity}, k-anonymity \cite{k-anon}), to guarantee data confidentiality and integrity (e.g., encryption \cite{thambiraja2012survey}, differential privacy \cite{hassan2019differential}, access control \cite{tolone2005access,servos2017current}), and to govern data sharing and analysis (e.g., data lineage \cite{woodruff1997supporting}, ETL/ELT ingestion \cite{vassiliadis2009survey}).


far notare che il lavori considerano 2 cose, da una parte tecniche per garantire sicurezza, dall'altra tecniche per garantire sicurezza in piattaforme per big data. In entrambi i casi molto specifiche.

An effective data governance and protection approach cannot avoid its integration within state-of-the-art big data infrastructures. In fact, as organizations see practical results and significant value in the usage of big data, they also recognize the limits of current big data ecosystems with respect to data governance and data protection. Recently, both industry and academic communities started to investigate the issue, both from a data governance perspective \cite{al2018exploring,aissa2020decide} or recognizing the need of new security requirements \cite{Colombo:JournCybersec:2019}.
There are also database-centric approaches that focus on specific databases such as noSQL databases or graph databases, or specific types of analytical pipelines such \cite{AConGraphDB:2021, AConMongoDB:2022, ABACforHBase:2019}. However, these solutions are widely based on query rewriting mechanisms leading to high complexity and low efficiency. Finally, some solutions are scenario-specific (federate cloud, edge microservices or IoT) and lack the generality needed to adapt to multiple contexts \cite{MultipartyAC:2019, IoTSecurity}. The closest approach to this project proposal is the work of Hu et al. \cite{ HUFerraiolo:2014}, introducing a generalized access control model for big data processing frameworks, which can be extended to the Hadoop environment. However, the paper discusses the issues only from a high-level architectural point of view, without discussing a tangible solution. Another relevant work is by Xue et al. \cite{GuardSpark:ACSAC:2020}. They propose a solution based on the notion of purpose-aware access control \cite{Byun2008} that, although focusing only on Apache Spark, recognizes the need of a generalized approach to deal with access control in analytics pipelines.
Platform-specific approaches are designed for single systems only (e.g., MongoDB, Hadoop) and leverage on native access control features of the platform \cite{rathore2017hadoop,anisetti2018privacy}.
Some recent proposals, like Federated Access Control Reference Model (FACRM) \cite{FederationAC:Journ:2020} or \cite{Sandhu:ABAC:2018,GuptaSandu:2017}, are specifically tailored to the Apache Hadoop stack.
On the other hand, platform-independent approaches have the advantage of being more general than platform-specific solutions. However, the currently available platforms either model resources to be accessed as monolithic files (e.g., Microsoft DAC) or lack scalability.

\cite{7014544} Principalmente orientato alla sicurezza, senza nessun considerazione della qualità del dato. Un mero modello di AC.
This paper proposes a general purpose AC scheme for distributed BD processing clusters.

\cite{10.1007/978-3-642-10665-1_11} dal punto di vista architetturale, dove si possono mettere i controlli

\cite{balancingInMedicine} However centralized data-storage has its pitfalls, especially regarding data privacy. We therefore drafted an IT infrastructure that uses decentralized storage to ensure data privacy, but still enables data transfer between participating hospitals. It implements an independent information broker to ensure anonymity of patients. Still it provides a way for researchers to request data and hospitals to contribute data on an opt-in basis. Although not an entirely new approach, the emphasis on data privacy throughout the design is a novel aspect providing a better balance between the need for big sample sizes and patient privacy. 

%%%%%%%%%%%%%%%%%%%
\subsection{Service Selection} 
%%%%%%%%%%%%%%%%%%%
citare TWEB
