\section{Related Work}\label{sec:related}


Given the breadth of topics covered, the related work is discussed separately to provide a more detailed and organized review.
In Section \ref{sec:dataquality} In Section \ref{sec:datagov} data governance solution security aware

%%%%%%%%%%%%%%%%%%%%
\subsection{Data protection and data quality}\label{sec:dataquality}
%%%%%%%%%%%%%%%%%%%%

Data quality is a largely studied research topic, from different communities and from different points of view. perspectives
Nell'ambito big data, principalmente Big data quality refers to the degree to which big data meets the requirements and expectations of its intended use. It encompasses various dimensions and characteristics to ensure that the data is reliable, accurate, and valuable for analysis and decision-making. Key dimensions of big data quality include:
    Accuracy: The correctness of the data, ensuring it accurately represents the real-world entities and events it is supposed to model.
    Consistency: The uniformity of the data across different datasets and over time, ensuring that data does not contradict itself.
    Validity: The data must conform to defined formats and constraints, ensuring it is suitable for the intended use.


Con l'avvento della necessità di proteggere i dati sensibili la questione della qualità ha iniziato anche a encompass a larger notion of accuracy, inteso nel senso di quanto sia valido. - Accuracy: it measures the proximity of a sanitized value to the original
value.
Per altro nell'ambito di Privacy Preserving Data Mining, il problema è opposto, perché si vuole calcolare la qualità del dato quality of data resulting from the anonymization process. data quality metrics are very important in the evaluation of PPDM
techniques.

e anche la necessità di avere delle metriche che misurino la qualità



In existing works, several data quality metrics have been proposed that are
either generic or data-use-specific \cite{Majeed2021AnonymizationTF}. METTERE ESEMPI E RIFERIMENTI PRESI DA LI However, currently, there is no metric that is widely accepted by the research community. The main problem with data quality is that its evaluation is relative  \cite{dataAccuracy,dataQuality}, in that it usually depends on the context in which data are used.
In the scientific literature data quality is generally
considered a multi-dimensional concept that in certain contexts involves
both objective and subjective parameters. Among the various possible
parameters, the following ones are usually considered the most relevant:
- Accuracy: it measures the proximity of a sanitized value to the original
value.
- Completeness: it evaluates the degree of missed data in the sanitized
database.
- Consistency: it is related to the internal constraints, that is, the relationships
that must hold among different fields of a data item or among data
items in a database.

Accuracy. The accuracy is closely related to the information loss resulting
from the hiding strategy: the less is the information loss, the better is the
data quality. This measure largely depends on the specific class of PPDM algorithms.
In what follows, we discuss how different approaches measure the
accuracy.


In our selection we tried to choose una che potesse andare bene. E soprattutto abbiamo cercato di fare delle considerazioni rispetto a tutto il ciclo di vita del dato

citare nostro journal


In evaluating the data quality after the privacy preserving process, it can be
useful to assess both the quality of the data resulting from the PPDM process


The database management research community mainly focused on increasing the quality of the source data rather than guaranteeing data quality along the whole processing pipeline or the quality of outcomes built on data.
In \cite{BigDataQaulitySurvey}, a survey on big data quality is proposed mentioning the well known categories of big data quality grouped by intrinsic, contextual representational and accessibility categories.
It also presents an holistic quality management model where the importance of data quality during processing is just mentioned in terms of requirements for the pre-processing job (e.g., data enhancement due to cleaning jobs).
In this paper we depart from this idea on data quality at pre processing time only measuring it at each step of the big data pipeline.

tecniche di crittografia
%\cite{8863330} 


cose simile a noi, nel senso che si rendono conto del problema e lo studiano
Privacy and analytics can work in tandem, but the mining outcome of a privacy aware design suffers from data quality
 in quello che segue ci sono anche in una tabella gli articoli citati
 NON SO SE METTERLI QUI O NELLA SEZIONE DOPO
 \cite{dataProtection}
 
Nel nostro caso abbiamo scelto due metriche, una di un tipo l'altra di un altro, ma il framework e l'approccio rimangono validi anche scegliendo un'altra metrica.

%%%%%%%%%%%%%%%%%%
\subsection{Data protection and data governance}\label{ssec:datagov}
%%%%%%%%%%%%%%%%%%

Research on data governance and protection focuses on the definition of new approaches and techniques aimed to protect the security and privacy of big data (e.g., CIA triad), as well as managing their life cycle with security and privacy in mind. Often, the research community is targeting specific security and privacy problems, resulting in a proliferation of solutions and tools, which are difficult to integrate in a coherent framework. Many solutions have been developed to protect the users' identity (e.g., anonymity \cite{wallace1999anonymity}, pseudonimity \cite{pfitzmann2001pseudonymity}, k-anonymity \cite{k-anon}), to guarantee data confidentiality and integrity (e.g., encryption \cite{thambiraja2012survey}, differential privacy \cite{hassan2019differential}, access control \cite{tolone2005access,servos2017current}), and to govern data sharing and analysis (e.g., data lineage \cite{woodruff1997supporting}, ETL/ELT ingestion \cite{vassiliadis2009survey}).


far notare che il lavori considerano 2 cose, da una parte tecniche per garantire sicurezza, dall'altra tecniche per garantire sicurezza in piattaforme per big data. In entrambi i casi molto specifiche.

An effective data governance and protection approach cannot avoid its integration within state-of-the-art big data infrastructures. In fact, as organizations see practical results and significant value in the usage of big data, they also recognize the limits of current big data ecosystems with respect to data governance and data protection. Recently, both industry and academic communities started to investigate the issue, both from a data governance perspective \cite{al2018exploring,aissa2020decide} or recognizing the need of new security requirements \cite{Colombo:JournCybersec:2019}.
There are also database-centric approaches that focus on specific databases such as noSQL databases or graph databases, or specific types of analytical pipelines such \cite{AConGraphDB:2021, AConMongoDB:2022, ABACforHBase:2019}. However, these solutions are widely based on query rewriting mechanisms leading to high complexity and low efficiency. Finally, some solutions are scenario-specific (federate cloud, edge microservices or IoT) and lack the generality needed to adapt to multiple contexts \cite{MultipartyAC:2019, IoTSecurity}. The closest approach to this project proposal is the work of Hu et al. \cite{ HUFerraiolo:2014}, introducing a generalized access control model for big data processing frameworks, which can be extended to the Hadoop environment. However, the paper discusses the issues only from a high-level architectural point of view, without discussing a tangible solution. Another relevant work is by Xue et al. \cite{GuardSpark:ACSAC:2020}. They propose a solution based on the notion of purpose-aware access control \cite{Byun2008} that, although focusing only on Apache Spark, recognizes the need of a generalized approach to deal with access control in analytics pipelines.
Platform-specific approaches are designed for single systems only (e.g., MongoDB, Hadoop) and leverage on native access control features of the platform \cite{rathore2017hadoop,anisetti2018privacy}.
Some recent proposals, like Federated Access Control Reference Model (FACRM) \cite{FederationAC:Journ:2020} or \cite{Sandhu:ABAC:2018,GuptaSandu:2017}, are specifically tailored to the Apache Hadoop stack.
On the other hand, platform-independent approaches have the advantage of being more general than platform-specific solutions. However, the currently available platforms either model resources to be accessed as monolithic files (e.g., Microsoft DAC) or lack scalability.

\cite{7014544} Principalmente orientato alla sicurezza, senza nessun considerazione della qualità del dato. Un mero modello di AC.
This paper proposes a general purpose AC scheme for distributed BD processing clusters.

\cite{10.1007/978-3-642-10665-1_11} dal punto di vista architetturale, dove si possono mettere i controlli

\cite{balancingInMedicine} However centralized data-storage has its pitfalls, especially regarding data privacy. We therefore drafted an IT infrastructure that uses decentralized storage to ensure data privacy, but still enables data transfer between participating hospitals. It implements an independent information broker to ensure anonymity of patients. Still it provides a way for researchers to request data and hospitals to contribute data on an opt-in basis. Although not an entirely new approach, the emphasis on data privacy throughout the design is a novel aspect providing a better balance between the need for big sample sizes and patient privacy. 

%%%%%%%%%%%%%%%%%%%
\subsection{Service Selection based on data quality} 
%%%%%%%%%%%%%%%%%%%
citare TWEB
