\section{Related Work}\label{sec:related}

Given the breadth of topics covered, the related work is discussed separately to provide a more detailed and organized review. In Section  \ref{sec:dataquality}, we address the issue of the lack of consensus on the definition of data quality and, consequently, on data quality metrics when applying data protection transformations. In Section \ref{sec:datagov}, we examine existing data governance solutions that tackle the problem of data protection when sharing data among different services. In Section \ref{sec:servicesel}, we review the literature on QoS (Quality of Service) service selection.

%%%%%%%%%%%%%%%%%%%%
\subsection{Data quality and data protection}\label{sec:dataquality}
%%%%%%%%%%%%%%%%%%%%

Data quality is a widely studied research topic across various communities and perspectives, such as the database community or when evaluating privacy preserving data mining techniques. In the context of big data, data quality primarily refers to the extent to which big data meets the requirements and expectations of its intended use, encompassing various dimensions and characteristics to ensure the data is reliable, accurate, and valuable for analysis and decision-making. Specifically, accuracy denotes the correctness of the data, ensuring it accurately represents the real-world entities and events it models.

With the increasing need to protect sensitive data, the notion of data quality has expanded to include a broader concept of accuracy, particularly in terms of the proximity of a sanitized value to the original value.
This shift has emphasized the necessity of metrics to assess the quality of data resulting from anonymization processes. Various data quality metrics have been proposed in existing literature, including generalized information loss (\textit{GenILoss}), discernability metric, minimal distortions, and average equivalence class size ($C_{AVG}$), which may either have broad applicability or be tailored to specific data scenarios \cite{Majeed2021AnonymizationTF,bookMetrics,reviewMetrics}. However, there is currently no metric that is widely accepted by the research community. The main challenge with data quality is its relative nature: its evaluation typically depends on the context in which the data is used and often involves both objective and subjective parameters \cite{dataAccuracy,dataQuality}.
%
A common consideration across all contexts is that accuracy is closely related to the information loss resulting from the anonymization strategy: the lower the information loss, the higher the data quality. In our scenario, we have opted for two generic metrics rooted in data loss assessment - one quantitative and one qualitative. Nonetheless, our framework and heuristic are designed to be modular and flexible, accommodating the chosen metric.

%%
Another critical consideration is the integration of data quality throughout the entire data lifecycle. Historically, the focus within the database management research community has predominantly been on enhancing the quality of source data, neglecting to ensure data quality across the whole processing pipeline, or the resulting outcomes. In \cite{BigDataQaulitySurvey}, %a survey on big data quality is proposed mentioning the well known categories of big data quality grouped by intrinsic, contextual representational and accessibility categories.
the authors propose a holistic quality management model that briefly considers data quality during processing, primarily in the context of prerequisites for preprocessing tasks (e.g., data refinement and enhancement through cleaning processes). In contrast, our approach diverges from this notion of assessing data quality solely during preprocessing, instead advocating for its evaluation at each stage of the big data pipeline. We build upon the evaluation conducted in \cite{impetusPaper} within a specific case study, wherein data protection transformations were implemented at each stage, albeit with only one candidate service.

%%%%%%%%%%%%%%%%%%
\subsection{Data governance and data protection}\label{sec:datagov}
%%%%%%%%%%%%%%%%%%

As organizations realize practical benefits and significant value from big data, they also acknowledge the limitations of current big data ecosystems, particularly regarding data governance and data protection, and the need for privacy-aware systems enforcing sensitive data protection. Recently, both industry and academic communities have begun to investigate the issue, recognizing the need of new security requirements \cite{Colombo:JournCybersec:2019} and the importance of addressing the conflict between the need to share and the need to protect information \cite{balancingact,VANDENBROEK2018330,balancingInMedicine,needtobalance,dataProtection}, from a data governance perspective \cite{al2018exploring,aissa2020decide}, and, more in general, to ensure compliance of the Big Data pipeline with generic non-functional requirements \cite{ABBJ.ICWS2022,ABHKKS.BD2023}.

Various proposals address data protection by implementing robust access control on big data platforms. Some approaches are platform-specific, tailored to single systems like MongoDB or Hadoop, and leverage the native access control features of these platforms \cite{rathore2017hadoop,anisetti2018privacy,FederationAC:Journ:2020,Sandhu:ABAC:2018,GuptaSandu:2017}. Other approaches focus on specific databases, such as NoSQL or graph databases, or specific types of analytical pipelines  \cite{AConGraphDB:2021, AConMongoDB:2022, ABACforHBase:2019}. However, these solutions often rely on query rewriting mechanisms, resulting in high complexity and low efficiency. Some solutions are designed for specific scenarios, such as federated cloud environments, edge microservices, or IoT, and lack the flexibility to adapt to multiple contexts \cite{MultipartyAC:2019, IoTSecurity}.

The most similar to our approach are platform-independent solutions that, like ours, adopt Attribute-Based Access Control (ABAC) \cite{XACML3.0} as a common underlying model, given its ability to support highly flexible and dynamic forms of data protection to business-critical data. They have the advantage of being more general than platform-specific solutions. However, the currently available platforms either model resources to be accessed as monolithic files (e.g., Microsoft DAC) or lack scalability. A relevant work by Hu et al.  \cite{ HUFerraiolo:2014} introduced a generalized access control model for big data processing frameworks that can be extended to the Hadoop environment. However, their work discusses the issues only from a high-level architectural perspective and does not offer a tangible solution or address data quality issues. Another relevant work by Xue et al.  \cite{GuardSpark:ACSAC:2020} proposes a solution based on purpose-aware access control \cite{Byun2008}, focusing on Apache Spark.
%
Our definition of pipeline template addresses the various challenges by allowing to express the security policies at the right level of granularity, considering individual services in the pipeline. It can also be easily mapped onto specific platforms, such as Apache-based systems, as we have demonstrated in \cite{medes2021}.

%%%%%%%%%%%%%%%%%%%
\subsection{Service Selection based on data quality}\label{sec:servicesel}
%%%%%%%%%%%%%%%%%%%

The selection and composition of services is a recent topic in the era of big data, originating from the Web service scenario but facing additional challenges due to the volume and velocity of data, as well as to the heterogeneity of services, domains, and hosting infrastructures. In the context of big data, Quality of Service (QoS) is particularly crucial for service selection: as organizations leverage vast amounts of data to enhance decision-making and operational efficiency, it is imperative to choose appropriate services for processing, analyzing, and interpreting this data. In particular, the selection process must account for both functional and non-functional requirements, including performance, scalability, reliability, and security standards. Despite its critical nature, security is often one of the least considered metrics in service selection \cite{SELLAMI2020102732}. Even when security is taken into account, it is not always evaluated in relation to data quality. 

Related works include \cite{secureWScomposition}, where Web services are composed according to the security requirements of both service requestors and providers. However, the range of expressible requirements is limited, such as the type of encryption algorithm or authentication method (e.g., SSO), and data sanitization is not considered. Thus, the selection algorithm is just a matching rather than a ranking with respect to a security metrics.

Another relevant study \cite{9844845} implements a certification-based service selection process, ranking services according to their certified non-functional properties and corresponding user requirements. In this approach, certified services are assumed to be functionally equivalent, offering the same functionality while meeting users' functional requirements.
 
The most related work to ours is \cite{SELLAMI2020102732}, where the authors address the challenges of big service composition, particularly QoS and security issues. Similarly to what we do with our pipeline template, they define a quality model for big services by extending the traditional QoS model of Web services to include ``big data"-related characteristics, and Quality of Data (QoD) attributes, such as completeness, accuracy, and timeliness. In order to address security issues, in their model, each service is assigned an L-Severity level \cite{Lseverity} that represents the potential severity of data leakages when consuming its data chunks. 
Their approach aims to select the optimal composition plan that not only maximizes QoS and QoD attributes such as timeliness (TL), completeness (CP), and consistency (CS), but it also minimizes L-Severity (LS), data sources and communication costs. 

%TWEB??